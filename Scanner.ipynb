{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from enum import Enum\n",
    "import re\n",
    "import pandas\n",
    "import pandastable as pt\n",
    "from nltk.tree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_type(Enum):  # listing all tokens type\n",
    "    program = 1\n",
    "    implicit = 2\n",
    "    none = 3\n",
    "    integer = 4\n",
    "    real = 5\n",
    "    Complex = 6\n",
    "    logical = 7\n",
    "    true = 8\n",
    "    Semicolon = 9\n",
    "    EqualOp = 10\n",
    "    LessThanOp = 11\n",
    "    GreaterThanOp = 12\n",
    "    NotEqualOp = 13\n",
    "    PlusOp = 14\n",
    "    MinusOp = 15\n",
    "    MultiplyOp = 16\n",
    "    DivideOp = 17\n",
    "    VarDeclOp = 18\n",
    "    character = 19\n",
    "    ExclMark = 20\n",
    "    parameter = 21\n",
    "    end = 22\n",
    "    If = 23\n",
    "    then = 24\n",
    "    Else = 25\n",
    "    do = 26\n",
    "    string = 27\n",
    "    read = 28\n",
    "    Print = 29\n",
    "    LessThanEqualOp = 30\n",
    "    GreaterThanEqualOp = 31\n",
    "    EqualEqualOp = 32\n",
    "    constant = 33\n",
    "    identifier = 34\n",
    "    Error = 35\n",
    "    Comma = 36\n",
    "    Len = 37\n",
    "    openParenthesis=38\n",
    "    closeParenthesis=39\n",
    "    false=40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserved word Dictionary\n",
    "ReservedWords = {\n",
    "    \"program\": Token_type.program,\n",
    "    \"implicit\": Token_type.implicit,\n",
    "    \"none\": Token_type.none,\n",
    "    \"end\": Token_type.end,\n",
    "    \"integer\": Token_type.integer,\n",
    "    \"real\": Token_type.real,\n",
    "    \"complex\": Token_type.Complex,\n",
    "    \"logical\": Token_type.logical,\n",
    "    \"character\": Token_type.character,\n",
    "    \"parameter\": Token_type.parameter,\n",
    "    \"if\": Token_type.If,\n",
    "    \"then\": Token_type.then,\n",
    "    \"else\": Token_type.Else,\n",
    "    \"do\": Token_type.do,\n",
    "    \"read\": Token_type.read,\n",
    "    \"print\": Token_type.Print,\n",
    "    \"len\": Token_type.Len,\n",
    "    \".true.\":Token_type.true,\n",
    "    \".false.\":Token_type.false\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Operators = {\n",
    "    # \".\": Token_type.Dot,\n",
    "    # \";\": Token_type.Semicolon,\n",
    "    \"=\": Token_type.EqualOp,\n",
    "    \"+\": Token_type.PlusOp,\n",
    "    \"-\": Token_type.MinusOp,\n",
    "    \"*\": Token_type.MultiplyOp,\n",
    "    \"/\": Token_type.DivideOp,\n",
    "    \"::\": Token_type.VarDeclOp,\n",
    "    \"!\": Token_type.ExclMark,\n",
    "    \">\": Token_type.GreaterThanOp,\n",
    "    \"<\": Token_type.LessThanOp,\n",
    "    \"<=\": Token_type.LessThanEqualOp,\n",
    "    \">=\": Token_type.GreaterThanEqualOp,\n",
    "    \"/=\": Token_type.NotEqualOp,\n",
    "    \"==\": Token_type.EqualEqualOp,\n",
    "    \",\": Token_type.Comma,\n",
    "    \"(\":Token_type.openParenthesis,\n",
    "    \")\":Token_type.closeParenthesis\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operator Precedence\n",
    "# '*'    '/'    '+'    '-'    '>'    '<'    '<='    '>='    '=='    '/='\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class token to hold string and token type\n",
    "class token:\n",
    "    def __init__(self, lex, token_type):\n",
    "        self.lex = lex\n",
    "        self.token_type = token_type\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'Lex': self.lex,\n",
    "            'token_type': self.token_type\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokens=[]\n",
    "Errors=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_token(text):\n",
    "    lexems = text.split()\n",
    "    for le in lexems:\n",
    "        if (le in ReservedWords):\n",
    "            new_token = token(le, ReservedWords[le])\n",
    "            Tokens.append(new_token)\n",
    "        elif (le in Operators):\n",
    "            new_token = token(le, Operators[le])\n",
    "            Tokens.append(new_token)\n",
    "        elif (re.match(\"^\\d+(\\.[0-9]*)?$\", le)):\n",
    "            new_token = token(le, Token_type.constant)\n",
    "            Tokens.append(new_token)\n",
    "        elif (re.match(\"^([a-zA-Z][a-zA-Z0-9]*)$\", le)):\n",
    "            new_token = token(le, Token_type.identifier)\n",
    "            Tokens.append(new_token)\n",
    "        elif (re.match(\"^\\\"[\\w. ]+\\\"$\", le) or re.match(\"^\\'[\\w. ]+\\'$\", le)):\n",
    "            new_token = token(le, Token_type.string)\n",
    "            Tokens.append(new_token)\n",
    "        else:\n",
    "            new_token = token(le, Token_type.Error)\n",
    "            Errors.append(\"Lexical error  \" + le)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scanner GUI\n",
    "root= tk.Tk()\n",
    "\n",
    "canvas1 = tk.Canvas(root, width=400, height=300, relief='raised')\n",
    "canvas1.pack()\n",
    "\n",
    "label1 = tk.Label(root, text='Scanner Phase')\n",
    "label1.config(font=('helvetica', 14))\n",
    "canvas1.create_window(200, 25, window=label1)\n",
    "\n",
    "label2 = tk.Label(root, text='Source code:')\n",
    "label2.config(font=('helvetica', 10))\n",
    "canvas1.create_window(200, 100, window=label2)\n",
    "\n",
    "entry1 = tk.Entry(root) \n",
    "canvas1.create_window(200, 140, window=entry1)\n",
    "\n",
    "def Scan():\n",
    "    x1 = entry1.get()\n",
    "    find_token(x1)\n",
    "    df=pandas.DataFrame.from_records([t.to_dict() for t in Tokens])\n",
    "    print(df)\n",
    "    label3 = tk.Label(root, text='Lexem ' + x1 + ' is:', font=('helvetica', 10))\n",
    "    canvas1.create_window(200, 210, window=label3)\n",
    "    \n",
    "    label4 = tk.Label(root, text=\"Token_type\"+x1, font=('helvetica', 10, 'bold'))\n",
    "    canvas1.create_window(200, 230, window=label4)\n",
    "    \n",
    "button1 = tk.Button(text='Scan', command=Scan, bg='brown', fg='white', font=('helvetica', 9, 'bold'))\n",
    "canvas1.create_window(200, 180, window=button1)\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
