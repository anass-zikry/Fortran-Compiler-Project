{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from enum import Enum\n",
    "import re\n",
    "import pandas\n",
    "import pandastable as pt\n",
    "from nltk.tree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_type(Enum):  # listing all tokens type\n",
    "    program = 1\n",
    "    implicit = 2\n",
    "    none = 3\n",
    "    integer = 4\n",
    "    real = 5\n",
    "    Complex = 6\n",
    "    logical = 7\n",
    "    true = 8\n",
    "    Semicolon = 9\n",
    "    EqualOp = 10\n",
    "    LessThanOp = 11\n",
    "    GreaterThanOp = 12\n",
    "    NotEqualOp = 13\n",
    "    PlusOp = 14\n",
    "    MinusOp = 15\n",
    "    MultiplyOp = 16\n",
    "    DivideOp = 17\n",
    "    VarDeclOp = 18\n",
    "    character = 19\n",
    "    ExclMark = 20\n",
    "    parameter = 21\n",
    "    end = 22\n",
    "    If = 23\n",
    "    then = 24\n",
    "    Else = 25\n",
    "    do = 26\n",
    "    string = 27\n",
    "    read = 28\n",
    "    Print = 29\n",
    "    LessThanEqualOp = 30\n",
    "    GreaterThanEqualOp = 31\n",
    "    EqualEqualOp = 32\n",
    "    constant = 33\n",
    "    identifier = 34\n",
    "    Error = 35\n",
    "    Comma = 36\n",
    "    Len = 37\n",
    "    openParenthesis=38\n",
    "    closeParenthesis=39\n",
    "    false=40\n",
    "    delimiter=41\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserved word Dictionary\n",
    "ReservedWords = {\n",
    "    \"program\": Token_type.program,\n",
    "    \"implicit\": Token_type.implicit,\n",
    "    \"none\": Token_type.none,\n",
    "    \"end\": Token_type.end,\n",
    "    \"integer\": Token_type.integer,\n",
    "    \"real\": Token_type.real,\n",
    "    \"complex\": Token_type.Complex,\n",
    "    \"logical\": Token_type.logical,\n",
    "    \"character\": Token_type.character,\n",
    "    \"parameter\": Token_type.parameter,\n",
    "    \"if\": Token_type.If,\n",
    "    \"then\": Token_type.then,\n",
    "    \"else\": Token_type.Else,\n",
    "    \"do\": Token_type.do,\n",
    "    \"read\": Token_type.read,\n",
    "    \"print\": Token_type.Print,\n",
    "    \"len\": Token_type.Len,\n",
    "    \".true.\":Token_type.true,\n",
    "    \".false.\":Token_type.false\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Operators = {\n",
    "    # \".\": Token_type.Dot,\n",
    "    # \";\": Token_type.Semicolon,\n",
    "    \"=\": Token_type.EqualOp,\n",
    "    \"+\": Token_type.PlusOp,\n",
    "    \"-\": Token_type.MinusOp,\n",
    "    \"*\": Token_type.MultiplyOp,\n",
    "    \"/\": Token_type.DivideOp,\n",
    "    \"::\": Token_type.VarDeclOp,\n",
    "    \"!\": Token_type.ExclMark,\n",
    "    \">\": Token_type.GreaterThanOp,\n",
    "    \"<\": Token_type.LessThanOp,\n",
    "    \"<=\": Token_type.LessThanEqualOp,\n",
    "    \">=\": Token_type.GreaterThanEqualOp,\n",
    "    \"/=\": Token_type.NotEqualOp,\n",
    "    \"==\": Token_type.EqualEqualOp,\n",
    "    \",\": Token_type.Comma,\n",
    "    \"(\":Token_type.openParenthesis,\n",
    "    \")\":Token_type.closeParenthesis,\n",
    "    \"\\n\":Token_type.delimiter\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operator Precedence\n",
    "# '*'    '/'    '+'    '-'    '>'    '<'    '<='    '>='    '=='    '/='\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class token to hold string and token type\n",
    "class token:\n",
    "    def __init__(self, lex, token_type):\n",
    "        self.lex = lex\n",
    "        self.token_type = token_type\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'Lex': self.lex,\n",
    "            'token_type': self.token_type\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokens=[]\n",
    "Errors=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "p=\"read  1 *, x  \"\n",
    "o=re.match(\"^\\s*(read)\\s*(\\*)\\s*(,)\\s*([a-zA-Z][a-zA-Z0-9]*)(\\s*(,)\\s*([a-zA-Z][a-zA-Z0-9]*))*\",p)\n",
    "# x=o.groups()\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_token(text):\n",
    "    for line in text :\n",
    "        re.match(\"^\\s*(read)\\s*(\\*)\\s*(,)\\s*([a-zA-Z][a-zA-Z0-9]*)(\\s*(,)\\s*([a-zA-Z][a-zA-Z0-9]*))*\",line)\n",
    "        lexems = line.split()\n",
    "        for le in lexems:\n",
    "            if (le == '!'):\n",
    "                new_token = token(le, Operators[le])\n",
    "                Tokens.append(new_token)\n",
    "                break  #üçé\n",
    "            elif (le in ReservedWords):\n",
    "                new_token = token(le, ReservedWords[le])\n",
    "                Tokens.append(new_token)\n",
    "            elif (le in Operators):\n",
    "                new_token = token(le, Operators[le])\n",
    "                Tokens.append(new_token)\n",
    "            elif (re.match(\"^\\d+(\\.[0-9]*)?$\", le)):\n",
    "                new_token = token(le, Token_type.constant)\n",
    "                Tokens.append(new_token)\n",
    "            elif (re.match(\"^([a-zA-Z][a-zA-Z0-9]*)$\", le)):\n",
    "                new_token = token(le, Token_type.identifier)\n",
    "                Tokens.append(new_token)\n",
    "            elif (re.match(\"^\\\"[\\w. ]+\\\"$\", le) or re.match(\"^\\'[\\w. ]+\\'$\", le)):\n",
    "                new_token = token(le, Token_type.string)\n",
    "                Tokens.append(new_token)\n",
    "            else:\n",
    "                new_token = token(le, Token_type.Error)\n",
    "                Errors.append(\"Lexical error  \" + le)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GUI\n",
    "root= tk.Tk()\n",
    "canvas1 = tk.Canvas(root, width=800, height=600, relief='raised')\n",
    "canvas1.pack()\n",
    "label1 = tk.Label(root, text='Scan & Parse this file')\n",
    "label1.config(font=('helvetica', 14))\n",
    "canvas1.create_window(350, 50, window=label1)\n",
    "label2 = tk.Label(root, text='Source code file path:')\n",
    "label2.config(font=('helvetica', 10))\n",
    "canvas1.create_window(300, 100, window=label2)\n",
    "entry1 = tk.Entry(root)\n",
    "canvas1.create_window(200, 140, window=entry1)\n",
    "\n",
    "def Scan():\n",
    "    filePath = entry1.get()\n",
    "    with open(filePath) as f:\n",
    "        lines=f.readlines()\n",
    "    print(lines)\n",
    "    find_token(lines)\n",
    "    df=pandas.DataFrame.from_records([t.to_dict() for t in Tokens])\n",
    "    #print(df)\n",
    "    \n",
    "    #to display token stream as table\n",
    "    dTDa1 = tk.Toplevel()\n",
    "    dTDa1.title('Token Stream')\n",
    "    dTDaPT = pt.Table(dTDa1, dataframe=df, showtoolbar=True, showstatusbar=True)\n",
    "    dTDaPT.show()\n",
    "    # start Parsing\n",
    "    # Node=Parse()\n",
    "    # Node=ProgramStart()\n",
    "     \n",
    "    \n",
    "    # to display errorlist\n",
    "    df1=pandas.DataFrame(Errors)\n",
    "    dTDa2 = tk.Toplevel()\n",
    "    dTDa2.title('Error List')\n",
    "    dTDaPT2 = pt.Table(dTDa2, dataframe=df1, showtoolbar=True, showstatusbar=True)\n",
    "    dTDaPT2.show()\n",
    "\n",
    "    # leaves=Node.leaves()\n",
    "    # print(\"Length:\"+str(len(leaves)))\n",
    "    # print(leaves)\n",
    "\n",
    "    # print(\"type:\"+ str(type(leaves[4])))\n",
    "    # Node.pop(0)\n",
    "    # print(Node.leaves())\n",
    "    # nn=Node.t\n",
    "    # print(nn)\n",
    "    # frfl=Tree.fromstring()\n",
    "    # for index,leaf in enumerate(leaves)  :\n",
    "    #     if leaf == None :\n",
    "    #         # print(\"NoneLeaf:\"+str(leaf))\n",
    "    #         treeIndex=Node.leaf_treeposition(index)\n",
    "    #         print(\"trI\"+str(treeIndex))\n",
    "    #         Node.pop([treeIndex])\n",
    "    #         # print(\"x=\"+str(x))\n",
    "    # Node.draw()\n",
    "    #clear your list\n",
    "    \n",
    "    #label3 = tk.Label(root, text='Lexem ' + x1 + ' is:', font=('helvetica', 10))\n",
    "    #canvas1.create_window(200, 210, window=label3)\n",
    "    \n",
    "    #label4 = tk.Label(root, text=\"Token_type\"+x1, font=('helvetica', 10, 'bold'))\n",
    "    #canvas1.create_window(200, 230, window=label4)\n",
    "    \n",
    "    \n",
    "button1 = tk.Button(text='Scan', command=Scan, bg='brown', fg='white', font=('helvetica', 9, 'bold'))\n",
    "canvas1.create_window(200, 180, window=button1)\n",
    "root.mainloop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "convert each line to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#üçé Header\n",
    "# üíÄ \"^\\s*(program)\\s+([a-z]\\w*)\\s*$\"gm\n",
    "#üçé implicit none\n",
    "# üíÄ \"^\\s*(implicit)\\s+(none)\\s*$\"gm\n",
    "#üçé type decleration without character\n",
    "# üíÄ \"^\\s*(integer|real|complex|logical)\\s*((,)\\s*(parameter))?\\s*(::)\\s*([a-z]\\w*)\\s*((,)\\s*([a-z]\\w*))*\\s*$\"gm\n",
    "#üçé character\n",
    "# üíÄ \"^\\s*(character)\\s*(\\(\\s*len\\s*=\\s*([a-z]\\w*|[1-9][0-9]*)\\s*\\))?\\s*(::)\\s*([a-z]\\w*)\\s*((,)\\s*([a-z]\\w*))*\\s*$\"gm\n",
    "#üçé Constant\n",
    "# üíÄ \"^\\s*(integer|real|complex|logical)\\s*((,)\\s*(parameter))\\s*(::)\\s*([a-z]\\w*)\\s*(=)\\s*((-|\\+)?[0-9]+|(-|\\+)?[0-9]+.?[0-9]*|((\\()\\s*(-|\\+)?[0-9]+.[0-9]+\\s*(,)\\s*(-|\\+)?[0-9]+.[0-9]+\\s*(\\)))|(.true.|.false.))\\s*$\"gm\n",
    "#üçé character constant\n",
    "# üíÄ \"^\\s*(character)\\s*(\\(\\s*len\\s*=\\s*([a-z]\\w*|[1-9][0-9]*)\\s*\\))?\\s*((,)\\s*(parameter))\\s*(::)\\s*([a-z]w*)\\s*(=)\\s*((')(\\w*)('))\\s*$\"gm\n",
    "#üçé read\n",
    "# üíÄ \"^\\s*(read)\\s*(\\*)\\s*(,)\\s*([a-z]\\w*)(\\s*(,)\\s*([a-z]\\w*))*\\s*$\"gm\n",
    "#üçé print\n",
    "# üíÄ \"^\\s*(print)\\s*(\\*)\\s*(,)\\s*([a-z]\\w*)?(\\s*(,)\\s*([a-z]\\w*))*\\s*$\"gm\n",
    "#üçé Assignment statement\n",
    "# üíÄ \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
