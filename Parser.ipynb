{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from enum import Enum\n",
    "import re\n",
    "import pandas\n",
    "import pandastable as pt\n",
    "from nltk.tree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_type(Enum):  # listing all tokens type\n",
    "    program = 1\n",
    "    implicit = 2\n",
    "    none = 3\n",
    "    integer = 4\n",
    "    real = 5\n",
    "    Complex = 6\n",
    "    logical = 7\n",
    "    true = 8\n",
    "    Semicolon = 9\n",
    "    EqualOp = 10\n",
    "    LessThanOp = 11\n",
    "    GreaterThanOp = 12\n",
    "    NotEqualOp = 13\n",
    "    PlusOp = 14\n",
    "    MinusOp = 15\n",
    "    MultiplyOp = 16\n",
    "    DivideOp = 17\n",
    "    VarDeclOp = 18\n",
    "    character = 19\n",
    "    ExclMark = 20\n",
    "    parameter = 21\n",
    "    end = 22\n",
    "    If = 23\n",
    "    then = 24\n",
    "    Else = 25\n",
    "    do = 26\n",
    "    string = 27\n",
    "    read = 28\n",
    "    Print = 29\n",
    "    LessThanEqualOp = 30\n",
    "    GreaterThanEqualOp = 31\n",
    "    EqualEqualOp = 32\n",
    "    constant = 33\n",
    "    identifier = 34\n",
    "    Error = 35\n",
    "    Comma = 36\n",
    "    Len = 37\n",
    "    openParenthesis=38\n",
    "    closeParenthesis=39\n",
    "    false=40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserved word Dictionary\n",
    "ReservedWords = {\n",
    "    \"program\": Token_type.program,\n",
    "    \"implicit\": Token_type.implicit,\n",
    "    \"none\": Token_type.none,\n",
    "    \"end\": Token_type.end,\n",
    "    \"integer\": Token_type.integer,\n",
    "    \"real\": Token_type.real,\n",
    "    \"complex\": Token_type.Complex,\n",
    "    \"logical\": Token_type.logical,\n",
    "    \"character\": Token_type.character,\n",
    "    \"parameter\": Token_type.parameter,\n",
    "    \"if\": Token_type.If,\n",
    "    \"then\": Token_type.then,\n",
    "    \"else\": Token_type.Else,\n",
    "    \"do\": Token_type.do,\n",
    "    \"read\": Token_type.read,\n",
    "    \"print\": Token_type.Print,\n",
    "    \"len\": Token_type.Len,\n",
    "    \".true.\":Token_type.true,\n",
    "    \".false.\":Token_type.false\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Operators = {\n",
    "    # \".\": Token_type.Dot,\n",
    "    # \";\": Token_type.Semicolon,\n",
    "    \"=\": Token_type.EqualOp,\n",
    "    \"+\": Token_type.PlusOp,\n",
    "    \"-\": Token_type.MinusOp,\n",
    "    \"*\": Token_type.MultiplyOp,\n",
    "    \"/\": Token_type.DivideOp,\n",
    "    \"::\": Token_type.VarDeclOp,\n",
    "    \"!\": Token_type.ExclMark,\n",
    "    \">\": Token_type.GreaterThanOp,\n",
    "    \"<\": Token_type.LessThanOp,\n",
    "    \"<=\": Token_type.LessThanEqualOp,\n",
    "    \">=\": Token_type.GreaterThanEqualOp,\n",
    "    \"/=\": Token_type.NotEqualOp,\n",
    "    \"==\": Token_type.EqualEqualOp,\n",
    "    \",\": Token_type.Comma,\n",
    "    \"(\":Token_type.openParenthesis,\n",
    "    \")\":Token_type.closeParenthesis\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operator Precedence\n",
    "# '*'    '/'    '+'    '-'    '>'    '<'    '<='    '>='    '=='    '/='\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class token to hold string and token type\n",
    "class token:\n",
    "    def __init__(self, lex, token_type):\n",
    "        self.lex = lex\n",
    "        self.token_type = token_type\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'Lex': self.lex,\n",
    "            'token_type': self.token_type\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokens=[]\n",
    "Errors=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_token(text):\n",
    "    lexems = text.split()\n",
    "    for le in lexems:\n",
    "        if (le in ReservedWords):\n",
    "            new_token = token(le, ReservedWords[le])\n",
    "            Tokens.append(new_token)\n",
    "        elif (le in Operators):\n",
    "            new_token = token(le, Operators[le])\n",
    "            Tokens.append(new_token)\n",
    "        elif (re.match(\"^\\d+(\\.[0-9]*)?$\", le)):\n",
    "            new_token = token(le, Token_type.constant)\n",
    "            Tokens.append(new_token)\n",
    "        elif (re.match(\"^([a-zA-Z][a-zA-Z0-9]*)$\", le)):\n",
    "            new_token = token(le, Token_type.identifier)\n",
    "            Tokens.append(new_token)\n",
    "        elif (re.match(\"^\\\"[\\w. ]+\\\"$\", le) or re.match(\"^\\'[\\w. ]+\\'$\", le)):\n",
    "            new_token = token(le, Token_type.string)\n",
    "            Tokens.append(new_token)\n",
    "        else:\n",
    "            new_token = token(le, Token_type.Error)\n",
    "            Errors.append(\"Lexical error  \" + le)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Match(TT,i) :\n",
    "    out = dict()\n",
    "    if(i<len(Tokens)) :\n",
    "        TokDict=Tokens[i].to_dict()\n",
    "        if(TokDict['token_type'] == TT) :\n",
    "            i+=1\n",
    "            out['node'] = [TokDict['Lex']]\n",
    "            out['index']=i\n",
    "            return out\n",
    "        else:\n",
    "            out['node']=['error']\n",
    "            out['index']=i+1\n",
    "            Errors.append(\"Syntax Error: \"+TokDict['Lex'])\n",
    "            return out\n",
    "    else :\n",
    "        out['node']=['error']\n",
    "        out['index']=i+1\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Parse():\n",
    "#     i = 0\n",
    "#     ProgramStartDict = ProgramStart(i)\n",
    "#     Parse_Node=Tree(\"\")\n",
    "#     return ProgramStartDict\n",
    "\n",
    "\n",
    "def ProgramStart():\n",
    "    i=0\n",
    "    # ProgramStart_Dict = dict()\n",
    "    ProgramStart_Children = []\n",
    "    Dict1 = ProgramUnit(i)\n",
    "    ProgramStart_Children.append(Dict1['node'])\n",
    "    Dict2 = ProgramStart2(Dict1['index'])\n",
    "    ProgramStart_Children.append(Dict2['node'])\n",
    "    ProgramStart_Node = Tree(\"ProgramStart\", ProgramStart_Children)\n",
    "    return ProgramStart_Node\n",
    "\n",
    "\n",
    "def ProgramStart2(i):\n",
    "    ProgramStart2_Dict = dict()\n",
    "    ProgramStart2_Children = []\n",
    "    if i < len(Tokens):\n",
    "        print(i)\n",
    "        Temp = Tokens[i].to_dict()\n",
    "        if Temp['token_type'] == Token_type.program:\n",
    "            Dict1 = ProgramUnit(i)\n",
    "            ProgramStart2_Children.append(Dict1['node'])\n",
    "            Dict2 = ProgramStart2(Dict1['index'])\n",
    "            ProgramStart2_Children.append(Dict2['node'])\n",
    "            ProgramStart2_Node = Tree(\"ProgramStart2\", ProgramStart2_Children)\n",
    "            ProgramStart2_Dict['node'] = ProgramStart2_Node\n",
    "            ProgramStart2_Dict['index'] = Dict2['index']\n",
    "            return ProgramStart2_Dict\n",
    "        else:\n",
    "            match1 = Match(Token_type.Error, i)\n",
    "            ProgramStart2_Children.append(match1['node'])\n",
    "            ProgramStart2_Node = Tree(\"ProgramStart2\", ProgramStart2_Children)\n",
    "            ProgramStart2_Dict['node'] = ProgramStart2_Node\n",
    "            ProgramStart2_Dict['index'] = match1['index']\n",
    "            return ProgramStart2_Dict\n",
    "    else:\n",
    "        ProgramStart2_Dict['node'] = None\n",
    "        ProgramStart2_Dict['index'] = i\n",
    "        return ProgramStart2_Dict\n",
    "\n",
    "\n",
    "def ProgramUnit(i):\n",
    "    ProgramUnit_dict = dict()\n",
    "    ProgramUnit_children = []\n",
    "    dict1 = Header(i)\n",
    "    ProgramUnit_children.append(dict1['node'])\n",
    "    dict2 = Block(dict1['index'])\n",
    "    ProgramUnit_children.append(dict2['node'])\n",
    "    dict3 = Footer(dict2['index'])\n",
    "    ProgramUnit_children.append(dict3['node'])\n",
    "    ProgramUnit_node = Tree(\"ProgramUnit\", ProgramUnit_children)\n",
    "    ProgramUnit_dict['node'] = ProgramUnit_node\n",
    "    ProgramUnit_dict['index'] = dict3['index']\n",
    "    return ProgramUnit_dict\n",
    "\n",
    "\n",
    "def Header(i):\n",
    "    Header_dict = dict()\n",
    "    Header_children = []\n",
    "    match1 = Match(Token_type.program, i)\n",
    "    Header_children.append(match1['node'])\n",
    "    match2 = Match(Token_type.identifier, match1['index'])\n",
    "    Header_children.append(match2['node'])\n",
    "    Header_node = Tree(\"Header\", Header_children)\n",
    "    Header_dict['node'] = Header_node\n",
    "    Header_dict['index'] = match2['index']\n",
    "    return Header_dict\n",
    "\n",
    "\n",
    "def Block(i):\n",
    "    Block_dict = dict()\n",
    "    Block_children = []\n",
    "    match1 = Match(Token_type.implicit, i)\n",
    "    Block_children.append(match1['node'])\n",
    "    match2 = Match(Token_type.none, match1['index'])\n",
    "    Block_children.append(match2['node'])\n",
    "    dict3 = TypeDecls(match2['index'])\n",
    "    Block_children.append(dict3['node'])\n",
    "    dict4 = Statements(dict3['index'])\n",
    "    Block_children.append(dict4['node'])\n",
    "    Block_node = Tree(\"Block\", Block_children)\n",
    "    Block_dict['node'] = Block_node\n",
    "    Block_dict['index'] = dict4['index']\n",
    "    return Block_dict\n",
    "\n",
    "\n",
    "def Footer(i):\n",
    "    Footer_dict = dict()\n",
    "    Footer_children = []\n",
    "    match1 = Match(Token_type.end, i)\n",
    "    Footer_children.append(match1['node'])\n",
    "    match2 = Match(Token_type.program, match1['index'])\n",
    "    Footer_children.append(match2['node'])\n",
    "    match3 = Match(Token_type.identifier, match2['index'])\n",
    "    Footer_children.append(match3['node'])\n",
    "    Footer_node = Tree(\"Footer\", Footer_children)\n",
    "    Footer_dict['node'] = Footer_node\n",
    "    Footer_dict['index'] = match3['index']\n",
    "    return Footer_dict\n",
    "\n",
    "\n",
    "def TypeDecls(i):\n",
    "    TypeDecls_dict = dict()\n",
    "    TypeDecls_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        Temp = Tokens[i].to_dict()\n",
    "        ############################################\n",
    "        if Temp['token_type'] in [Token_type.integer, Token_type.real, Token_type.Complex, Token_type.logical, Token_type.character]:\n",
    "            dict1 = TypeDecl(i)\n",
    "            TypeDecls_children.append(dict1['node'])\n",
    "            dict2 = TypeDecls2(dict1['index'])\n",
    "            TypeDecls_children.append(dict2['node'])\n",
    "            last_index = dict2['index']\n",
    "        else:\n",
    "            match1 = Match(Token_type.Error, i)\n",
    "            TypeDecls_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        TypeDecls_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "\n",
    "    TypeDecls_node = Tree(\"TypeDecls\", TypeDecls_children)\n",
    "    TypeDecls_dict['node'] = TypeDecls_node\n",
    "    TypeDecls_dict['index'] = last_index\n",
    "    return TypeDecls_dict\n",
    "\n",
    "\n",
    "def TypeDecls2(i):\n",
    "    TypeDecls2_dict = dict()\n",
    "    TypeDecls2_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        Temp = Tokens[i].to_dict()\n",
    "        ############################################\n",
    "        if Temp['token_type'] in [Token_type.integer, Token_type.real, Token_type.Complex, Token_type.logical, Token_type.character]:\n",
    "            dict1 = TypeDecl(i)\n",
    "            TypeDecls2_children.append(dict1['node'])\n",
    "            dict2 = TypeDecls2(dict1['index'])\n",
    "            TypeDecls2_children.append(dict2['node'])\n",
    "            last_index = dict2['index']\n",
    "        else:\n",
    "            TypeDecls2_dict['node'] = None\n",
    "            TypeDecls2_dict['index'] = i\n",
    "            return TypeDecls2_dict\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        TypeDecls2_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "\n",
    "    TypeDecls2_node = Tree(\"TypeDecls2\", TypeDecls2_children)\n",
    "    TypeDecls2_dict['node'] = TypeDecls2_node\n",
    "    TypeDecls2_dict['index'] = last_index\n",
    "    return TypeDecls2_dict\n",
    "\n",
    "\n",
    "def TypeDecl(i):\n",
    "    TypeDecl_dict = dict()\n",
    "    TypeDecl_children = []\n",
    "    dict1 = DataType(i)\n",
    "    TypeDecl_children.append(dict1['node'])\n",
    "    dict2 = TypeDecl2(dict1['index'])\n",
    "    TypeDecl_children.append(dict2['node'])\n",
    "    TypeDecl_node = Tree(\"TypeDecl\", TypeDecl_children)\n",
    "    TypeDecl_dict['node'] = TypeDecl_node\n",
    "    TypeDecl_dict['index'] = dict2['index']\n",
    "    return TypeDecl_dict\n",
    "\n",
    "\n",
    "def TypeDecl2(i):\n",
    "    TypeDecl2_dict = dict()\n",
    "    TypeDecl2_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        if temp['token_type'] == Token_type.VarDeclOp:\n",
    "            match1 = Match(Token_type.VarDeclOp, i)\n",
    "            TypeDecl2_children.append(match1['node'])\n",
    "            dict2 = IdentifierList(match1['index'])\n",
    "            TypeDecl2_children.append(dict2['node'])\n",
    "            last_index = dict2['index']\n",
    "        elif temp['token_type'] == Token_type.Comma:\n",
    "            match1 = Match(Token_type.Comma, i)\n",
    "            TypeDecl2_children.append(match1['node'])\n",
    "            match2 = Match(Token_type.parameter, match1['index'])\n",
    "            TypeDecl2_children.append(match2['node'])\n",
    "            match3 = Match(Token_type.VarDeclOp, match2['index'])\n",
    "            TypeDecl2_children.append(match3['node'])\n",
    "            dict4 = NamedConstant(match3['index'])\n",
    "            TypeDecl2_children.append(dict4['node'])\n",
    "            last_index = dict4['index']\n",
    "        else:\n",
    "            match1 = Match(Token_type.Error, i)\n",
    "            TypeDecl2_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "    TypeDecl2_node = Tree(\"TypeDecl2\", TypeDecl2_children)\n",
    "    TypeDecl2_dict['node'] = TypeDecl2_node\n",
    "    TypeDecl2_dict['index'] = last_index\n",
    "    return TypeDecl2_dict\n",
    "\n",
    "\n",
    "def DataType(i):\n",
    "    DataType_dict = dict()\n",
    "    DataType_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        if temp['token_type'] == Token_type.integer:\n",
    "            match1 = Match(Token_type.integer, i)\n",
    "            DataType_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] == Token_type.real:\n",
    "            match1 = Match(Token_type.real, i)\n",
    "            DataType_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] == Token_type.Complex:\n",
    "            match1 = Match(Token_type.Complex, i)\n",
    "            DataType_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] == Token_type.logical:\n",
    "            match1 = Match(Token_type.logical, i)\n",
    "            DataType_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] == Token_type.character:\n",
    "            dict1 = CharacterDType(i)\n",
    "            DataType_children.append(dict1['node'])\n",
    "            last_index = dict1['index']\n",
    "        else:\n",
    "            match1 = Match(Token_type.Error, i)\n",
    "            DataType_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        DataType_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    DataType_node = Tree(\"DataType\", DataType_children)\n",
    "    DataType_dict['node'] = DataType_node\n",
    "    DataType_dict['index'] = last_index\n",
    "    return DataType_dict\n",
    "\n",
    "\n",
    "def NamedConstant(i):\n",
    "    NamedConstant_dict = dict()\n",
    "    NamedConstant_children = []\n",
    "    match1 = Match(Token_type.identifier, i)\n",
    "    NamedConstant_children.append(match1['node'])\n",
    "    match2 = Match(Token_type.EqualOp, match1['index'])\n",
    "    NamedConstant_children.append(match2['node'])\n",
    "    dict3 = LogicalOrConst(match2['index'])\n",
    "    NamedConstant_children.append(dict3['node'])\n",
    "    NamedConstant_node = Tree(\"NamedConstant\", NamedConstant_children)\n",
    "    NamedConstant_dict['node'] = NamedConstant_node\n",
    "    NamedConstant_dict['index'] = dict3['index']\n",
    "    return NamedConstant_dict\n",
    "\n",
    "\n",
    "def CharacterDType(i):\n",
    "    CharacterDType_dict = dict()\n",
    "    CharacterDType_children = []\n",
    "    match1 = Match(Token_type.character, i)\n",
    "    CharacterDType_children.append(match1['node'])\n",
    "    dict2 = CharacterDType2(match1['index'])\n",
    "    CharacterDType_children.append(dict2['node'])\n",
    "    CharacterDType_node = Tree(\"CharacterDType\", CharacterDType_children)\n",
    "    CharacterDType_dict['node'] = CharacterDType_node\n",
    "    CharacterDType_dict['index'] = dict2['index']\n",
    "    return CharacterDType_dict\n",
    "\n",
    "\n",
    "def CharacterDType2(i):\n",
    "    CharacterDType2_dict = dict()\n",
    "    CharacterDType2_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        if temp['token_type'] == Token_type.openParenthesis:\n",
    "            match1 = Match(Token_type.openParenthesis, i)\n",
    "            CharacterDType2_children.append(match1['node'])\n",
    "            match2 = Match(Token_type.Len, match1['index'])\n",
    "            CharacterDType2_children.append(match2['node'])\n",
    "            match3 = Match(Token_type.EqualOp, match2['index'])\n",
    "            CharacterDType2_children.append(match3['node'])\n",
    "            dict4 = IdorConst(match3['index'])\n",
    "            CharacterDType2_children.append(dict4['node'])\n",
    "            match5 = Match(Token_type.closeParenthesis, dict4['index'])\n",
    "            CharacterDType2_children.append(match5['node'])\n",
    "            last_index = match5['index']\n",
    "        else:\n",
    "            CharacterDType2_dict['node'] = None\n",
    "            CharacterDType2_dict['index'] = i\n",
    "            return CharacterDType2_dict\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        CharacterDType2_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    CharacterDType2_node = Tree(\"CharacterDType2\", CharacterDType2_children)\n",
    "    CharacterDType2_dict['node'] = CharacterDType2_node\n",
    "    CharacterDType2_dict['index'] = last_index\n",
    "    return CharacterDType2_dict\n",
    "\n",
    "\n",
    "def Statements(i):\n",
    "    Statements_dict = dict()\n",
    "    Statements_children = []\n",
    "    dict1 = Statement(i)\n",
    "    Statements_children.append(dict1['node'])\n",
    "    dict2 = Statements2(dict1['index'])\n",
    "    Statements_children.append(dict2['node'])\n",
    "    Statements_node = Tree(\"Statements\", Statements_children)\n",
    "    Statements_dict['node'] = Statements_node\n",
    "    Statements_dict['index'] = dict2['index']\n",
    "    return Statements_dict\n",
    "\n",
    "\n",
    "def Statements2(i):\n",
    "    Statements2_dict = dict()\n",
    "    Statements2_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        #################################################\n",
    "        if temp['token_type'] in [Token_type.identifier, Token_type.Print, Token_type.read, Token_type.If, Token_type.do]:\n",
    "            dict1 = Statement(i)\n",
    "            Statements2_children.append(dict1['node'])\n",
    "            dict2 = Statements2(dict1['index'])\n",
    "            Statements2_children.append(dict2['node'])\n",
    "            last_index = dict2['index']\n",
    "\n",
    "        else:\n",
    "            Statements2_dict['node'] = None\n",
    "            Statements2_dict['index'] = i\n",
    "            return Statements2_dict\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        Statements2_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    Statements2_node = Tree(\"Statements2\", Statements2_children)\n",
    "    Statements2_dict['node'] = Statements2_node\n",
    "    Statements2_dict['index'] = last_index\n",
    "    return Statements2_dict\n",
    "\n",
    "\n",
    "def Statement(i):\n",
    "    Statement_dict = dict()\n",
    "    Statement_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        if temp['token_type'] == Token_type.identifier:\n",
    "            dict1 = Assignment(i)\n",
    "            Statement_children.append(dict1['node'])\n",
    "            last_index = dict1['index']\n",
    "        elif temp['token_type'] == Token_type.Print:\n",
    "            dict1 = Print(i)\n",
    "            Statement_children.append(dict1['node'])\n",
    "            last_index = dict1['index']\n",
    "        elif temp['token_type'] == Token_type.read:\n",
    "            dict1 = Read(i)\n",
    "            Statement_children.append(dict1['node'])\n",
    "            last_index = dict1['index']\n",
    "        elif temp['token_type'] == Token_type.If:\n",
    "            dict1 = If(i)\n",
    "            Statement_children.append(dict1['node'])\n",
    "            last_index = dict1['index']\n",
    "        elif temp['token_type'] == Token_type.do:\n",
    "            dict1 = DoLoop(i)\n",
    "            Statement_children.append(dict1['node'])\n",
    "            last_index = dict1['index']\n",
    "        else:\n",
    "            Statement_dict['node'] = None\n",
    "            Statement_dict['index'] = i\n",
    "            return Statement_dict\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        Statement_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    Statement_node = Tree(\"Statement\", Statement_children)\n",
    "    Statement_dict['node'] = Statement_node\n",
    "    Statement_dict['index'] = last_index\n",
    "    return Statement_dict\n",
    "\n",
    "\n",
    "def Assignment(i):\n",
    "    Assignment_dict = dict()\n",
    "    Assignment_children = []\n",
    "    match1 = Match(Token_type.identifier, i)\n",
    "    Assignment_children.append(match1['node'])\n",
    "    match2 = Match(Token_type.EqualOp, match1['index'])\n",
    "    Assignment_children.append(match2['node'])\n",
    "    dict3 = Relations(match2['index'])\n",
    "    Assignment_children.append(dict3['node'])\n",
    "    Assignment_node = Tree(\"Assignment\", Assignment_children)\n",
    "    Assignment_dict['node'] = Assignment_node\n",
    "    Assignment_dict['index'] = dict3['index']\n",
    "    return Assignment_dict\n",
    "\n",
    "\n",
    "def Relations(i):\n",
    "    Relations_dict = dict()\n",
    "    Relations_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        #################################################\n",
    "        if temp['token_type'] in [Token_type.identifier, Token_type.constant]:\n",
    "            dict1 = IdorConst(i)\n",
    "            Relations_children.append(dict1['node'])\n",
    "            dict2 = Relation(dict1['index'])\n",
    "            Relations_children.append(dict2['node'])\n",
    "            last_index = dict2['index']\n",
    "        elif temp['token_type'] in [Token_type.true, Token_type.false]:\n",
    "            dict1 = LogicalVal(i)\n",
    "            Relations_children.append(dict1['node'])\n",
    "            last_index = dict1['index']\n",
    "\n",
    "        else:\n",
    "            match1 = Match(Token_type.Error, i)\n",
    "            Relations_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        Relations_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    Relations_node = Tree(\"Relations\", Relations_children)\n",
    "    Relations_dict['node'] = Relations_node\n",
    "    Relations_dict['index'] = last_index\n",
    "    return Relations_dict\n",
    "\n",
    "\n",
    "def Relation(i):\n",
    "    Relation_dict = dict()\n",
    "    Relation_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        #################################################\n",
    "        if temp['token_type'] in [Token_type.PlusOp, Token_type.MultiplyOp, Token_type.MinusOp, Token_type.DivideOp]:\n",
    "            dict1 = ArithmeticOp(i)\n",
    "            Relation_children.append(dict1['node'])\n",
    "            dict2 = IdorConst(dict1['index'])\n",
    "            Relation_children.append(dict2['node'])\n",
    "            dict3 = Relation2(dict2['index'])\n",
    "            Relation_children.append(dict3['node'])\n",
    "            last_index = dict3['index']\n",
    "\n",
    "        else:\n",
    "            Relation_dict['node'] = None\n",
    "            Relation_dict['index'] = i\n",
    "            return Relation_dict\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        Relation_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    Relation_node = Tree(\"Relation\", Relation_children)\n",
    "    Relation_dict['node'] = Relation_node\n",
    "    Relation_dict['index'] = last_index\n",
    "    return Relation_dict\n",
    "\n",
    "\n",
    "def Relation2(i):\n",
    "    Relation2_dict = dict()\n",
    "    Relation2_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        #################################################\n",
    "        if temp['token_type'] in [Token_type.PlusOp, Token_type.MultiplyOp, Token_type.MinusOp, Token_type.DivideOp]:\n",
    "            dict1 = ArithmeticOp(i)\n",
    "            Relation2_children.append(dict1['node'])\n",
    "            dict2 = IdorConst(dict1['index'])\n",
    "            Relation2_children.append(dict2['node'])\n",
    "            dict3 = Relation2(dict2['index'])\n",
    "            Relation2_children.append(dict3['node'])\n",
    "            last_index = dict3['index']\n",
    "\n",
    "        else:\n",
    "            Relation2_dict['node'] = None\n",
    "            Relation2_dict['index'] = i\n",
    "            return Relation2_dict\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        Relation2_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    Relation_node = Tree(\"Relation\", Relation2_children)\n",
    "    Relation2_dict['node'] = Relation_node\n",
    "    Relation2_dict['index'] = last_index\n",
    "    return Relation2_dict\n",
    "\n",
    "\n",
    "def Print(i):\n",
    "    Print_dict = dict()\n",
    "    Print_children = []\n",
    "    match1 = Match(Token_type.Print, i)\n",
    "    Print_children.append(match1['node'])\n",
    "    match2 = Match(Token_type.MultiplyOp, match1['index'])\n",
    "    Print_children.append(match2['node'])\n",
    "    dict3 = PrintCall(match2['index'])\n",
    "    Print_children.append(dict3['node'])\n",
    "    Print_node = Tree(\"Print\", Print_children)\n",
    "    Print_dict['node'] = Print_node\n",
    "    Print_dict['index'] = dict3['index']\n",
    "    return Print_dict\n",
    "\n",
    "\n",
    "def PrintCall(i):\n",
    "    PrintCall_dict = dict()\n",
    "    PrintCall_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        #################################################\n",
    "        if temp['token_type'] == Token_type.Comma:\n",
    "            match1 = Match(Token_type.Comma, i)\n",
    "            PrintCall_children.append(match1['node'])\n",
    "            dict2 = PrintList(match1['index'])\n",
    "            PrintCall_children.append(dict2['node'])\n",
    "            last_index = dict2['index']\n",
    "\n",
    "        else:\n",
    "            PrintCall_dict['node'] = None\n",
    "            PrintCall_dict['index'] = i\n",
    "            return PrintCall_dict\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        PrintCall_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    PrintCall_node = Tree(\"PrintCall\", PrintCall_children)\n",
    "    PrintCall_dict['node'] = PrintCall_node\n",
    "    PrintCall_dict['index'] = last_index\n",
    "    return PrintCall_dict\n",
    "\n",
    "\n",
    "def PrintList(i):\n",
    "    PrintList_dict = dict()\n",
    "    PrintList_children = []\n",
    "    dict1 = PrintHolder(i)\n",
    "    PrintList_children.append(dict1['node'])\n",
    "    dict2 = PrintList2(dict1['index'])\n",
    "    PrintList_children.append(dict2['node'])\n",
    "    PrintList_node = Tree(\"PrintList\", PrintList_children)\n",
    "    PrintList_dict['node'] = PrintList_node\n",
    "    PrintList_dict['index'] = dict2['index']\n",
    "    return PrintList_dict\n",
    "\n",
    "\n",
    "def PrintList2(i):\n",
    "    PrintList2_dict = dict()\n",
    "    PrintList2_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        #################################################\n",
    "        if temp['token_type'] == Token_type.Comma:\n",
    "            match1 = Match(Token_type.Comma, i)\n",
    "            PrintList2_children.append(match1['node'])\n",
    "            dict2 = PrintHolder(match1['index'])\n",
    "            PrintList2_children.append(dict2['node'])\n",
    "            dict3 = PrintList2(dict2['index'])\n",
    "            PrintList2_children.append(dict3['node'])\n",
    "            last_index = dict3['index']\n",
    "\n",
    "        else:\n",
    "            PrintList2_dict['node'] = None\n",
    "            PrintList2_dict['index'] = i\n",
    "            return PrintList2_dict\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        PrintList2_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    PrintList2_node = Tree(\"PrintList2\", PrintList2_children)\n",
    "    PrintList2_dict['node'] = PrintList2_node\n",
    "    PrintList2_dict['index'] = last_index\n",
    "    return PrintList2_dict\n",
    "\n",
    "\n",
    "def PrintHolder(i):\n",
    "    PrintHolder_dict = dict()\n",
    "    PrintHolder_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        #################################################\n",
    "        if temp['token_type'] == Token_type.identifier:\n",
    "            match1 = Match(Token_type.identifier, i)\n",
    "            PrintHolder_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] == Token_type.constant:\n",
    "            match1 = Match(Token_type.constant, i)\n",
    "            PrintHolder_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] == Token_type.string:\n",
    "            match1 = Match(Token_type.string, i)\n",
    "            PrintHolder_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        else:\n",
    "            PrintHolder_dict['node'] = None\n",
    "            PrintHolder_dict['index'] = i\n",
    "            return PrintHolder_dict\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        PrintHolder_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    PrintHolder_node = Tree(\"PrintHolder\", PrintHolder_children)\n",
    "    PrintHolder_dict['node'] = PrintHolder_node\n",
    "    PrintHolder_dict['index'] = last_index\n",
    "    return PrintHolder_dict\n",
    "\n",
    "\n",
    "def Read(i):\n",
    "    Read_dict = dict()\n",
    "    Read_children = []\n",
    "    match1 = Match(Token_type.read, i)\n",
    "    Read_children.append(match1['node'])\n",
    "    match2 = Match(Token_type.MultiplyOp, match1['index'])\n",
    "    Read_children.append(match2['node'])\n",
    "    match3 = Match(Token_type.Comma, match2['index'])\n",
    "    Read_children.append(match3['node'])\n",
    "    dict4 = IdentifierList(match3['index'])\n",
    "    Read_children.append(dict4['node'])\n",
    "    Read_node = Tree(\"Read\", Read_children)\n",
    "    Read_dict['node'] = Read_node\n",
    "    Read_dict['index'] = dict4['index']\n",
    "    return Read_dict\n",
    "\n",
    "\n",
    "def If(i):\n",
    "    If_dict = dict()\n",
    "    If_children = []\n",
    "    dict1 = IfStart(i)\n",
    "    If_children.append(dict1['node'])\n",
    "    dict2 = If2(dict1['index'])\n",
    "    If_children.append(dict2['node'])\n",
    "    If_node = Tree(\"If\", If_children)\n",
    "    If_dict['node'] = If_node\n",
    "    If_dict['index'] = dict2['index']\n",
    "    return If_dict\n",
    "\n",
    "\n",
    "def If2(i):\n",
    "    If2_dict = dict()\n",
    "    If2_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        #################################################\n",
    "        if temp['token_type'] == Token_type.end:\n",
    "            match1 = Match(Token_type.end, i)\n",
    "            If2_children.append(match1['node'])\n",
    "            match2 = Match(Token_type.If, match1['index'])\n",
    "            If2_children.append(match2['node'])\n",
    "            last_index = match2['index']\n",
    "        elif temp['token_type'] == Token_type.Else:\n",
    "            match1 = Match(Token_type.Else, i)\n",
    "            If2_children.append(match1['node'])\n",
    "            dict2 = Statements(match1['index'])\n",
    "            If2_children.append(dict2['node'])\n",
    "            match3 = Match(Token_type.end, i)\n",
    "            If2_children.append(match3['node'])\n",
    "            match4 = Match(Token_type.If, match3['index'])\n",
    "            If2_children.append(match4['node'])\n",
    "            last_index = match4['index']\n",
    "        else:\n",
    "            match1 = Match(Token_type.Error, i)\n",
    "            If2_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        If2_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    If2_node = Tree(\"If2\", If2_children)\n",
    "    If2_dict['node'] = If2_node\n",
    "    If2_dict['index'] = last_index\n",
    "    return If2_dict\n",
    "\n",
    "\n",
    "def IfStart(i):\n",
    "    IfStart_dict = dict()\n",
    "    IfStart_children = []\n",
    "    match1 = Match(Token_type.If, i)\n",
    "    IfStart_children.append(match1['node'])\n",
    "    match2 = Match(Token_type.openParenthesis, match1['index'])\n",
    "    IfStart_children.append(match2['node'])\n",
    "    dict3 = Conditional(match2['index'])\n",
    "    IfStart_children.append(dict3['node'])\n",
    "    match4 = Match(Token_type.closeParenthesis, dict3['index'])\n",
    "    IfStart_children.append(match4['node'])\n",
    "    match5 = Match(Token_type.then, match4['index'])\n",
    "    IfStart_children.append(match5['node'])\n",
    "    dict6 = Statements(match5['index'])\n",
    "    IfStart_children.append(dict6['node'])\n",
    "    IfStart_node = Tree(\"IfStart\", IfStart_children)\n",
    "    IfStart_dict['node'] = IfStart_node\n",
    "    IfStart_dict['index'] = dict6['index']\n",
    "    return IfStart_dict\n",
    "\n",
    "\n",
    "def DoLoop(i):\n",
    "    DoLoop_dict = dict()\n",
    "    DoLoop_children = []\n",
    "    dict1 = DoStart(i)\n",
    "    DoLoop_children.append(dict1['node'])\n",
    "    dict2 = Statements(dict1['index'])\n",
    "    DoLoop_children.append(dict2['node'])\n",
    "    match3 = Match(Token_type.end, dict2['index'])\n",
    "    DoLoop_children.append(match3['node'])\n",
    "    match4 = Match(Token_type.do, match3['index'])\n",
    "    DoLoop_children.append(match4['node'])\n",
    "    DoLoop_node = Tree(\"DoLoop\", DoLoop_children)\n",
    "    DoLoop_dict['node'] = DoLoop_node\n",
    "    DoLoop_dict['index'] = match4['index']\n",
    "    return DoLoop_dict\n",
    "\n",
    "\n",
    "def DoStart(i):\n",
    "    DoStart_dict = dict()\n",
    "    DoStart_children = []\n",
    "    match1 = Match(Token_type.do, i)\n",
    "    DoStart_children.append(match1['node'])\n",
    "    match2 = Match(Token_type.identifier, match1['index'])\n",
    "    DoStart_children.append(match2['node'])\n",
    "    match3 = Match(Token_type.EqualOp, match2['index'])\n",
    "    DoStart_children.append(match3['node'])\n",
    "    dict4 = IdorConst(match3['index'])\n",
    "    DoStart_children.append(dict4['node'])\n",
    "    match5 = Match(Token_type.Comma, dict4['index'])\n",
    "    DoStart_children.append(match5['node'])\n",
    "    dict6 = IdorConst(match5['index'])\n",
    "    DoStart_children.append(dict6['node'])\n",
    "    dict7 = Step(dict6['index'])\n",
    "    DoStart_children.append(dict7['node'])\n",
    "    DoStart_node = Tree(\"DoStart\", DoStart_children)\n",
    "    DoStart_dict['node'] = DoStart_node\n",
    "    DoStart_dict['index'] = dict7['index']\n",
    "    return DoStart_dict\n",
    "\n",
    "\n",
    "def Step(i):\n",
    "    Step_dict = dict()\n",
    "    Step_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        if temp['token_type'] == Token_type.Comma:\n",
    "            match1 = Match(Token_type.Comma, i)\n",
    "            Step_children.append(match1['node'])\n",
    "            dict2 = IdorConst(match1['index'])\n",
    "            Step_children.append(dict2['node'])\n",
    "            last_index = dict2['index']\n",
    "        else:\n",
    "            Step_dict['node'] = None\n",
    "            Step_dict['index'] = i\n",
    "            return Step_dict\n",
    "\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        Step_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    Step_node = Tree(\"Step\", Step_children)\n",
    "    Step_dict['node'] = Step_node\n",
    "    Step_dict['index'] = last_index\n",
    "    return Step_dict\n",
    "\n",
    "\n",
    "def ArithmeticOp(i):\n",
    "    ArithmeticOp_dict = dict()\n",
    "    ArithmeticOp_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        if temp['token_type'] == Token_type.MultiplyOp:\n",
    "            match1 = Match(Token_type.MultiplyOp, i)\n",
    "            ArithmeticOp_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] == Token_type.DivideOp:\n",
    "            match1 = Match(Token_type.DivideOp, i)\n",
    "            ArithmeticOp_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] == Token_type.PlusOp:\n",
    "            match1 = Match(Token_type.PlusOp, i)\n",
    "            ArithmeticOp_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] == Token_type.MinusOp:\n",
    "            match1 = Match(Token_type.MinusOp, i)\n",
    "            ArithmeticOp_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        else:\n",
    "            match1 = Match(Token_type.Error, i)\n",
    "            ArithmeticOp_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        ArithmeticOp_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    ArithmeticOp_node = Tree(\"ArithmeticOp\")\n",
    "    ArithmeticOp_dict['node'] = ArithmeticOp_node\n",
    "    ArithmeticOp_dict['index']\n",
    "    return ArithmeticOp_dict\n",
    "\n",
    "\n",
    "def IdentifierList(i):\n",
    "    IdentifierList_dict = dict()\n",
    "    IdentifierList_children = []\n",
    "    match1 = Match(Token_type.identifier, i)\n",
    "    IdentifierList_children.append(match1['node'])\n",
    "    dict2 = IdentifierList2(match1['index'])\n",
    "    IdentifierList_children.append(dict2['node'])\n",
    "    IdentifierList_node = Tree(\"IdentifierList\", IdentifierList_children)\n",
    "    IdentifierList_dict['node'] = IdentifierList_node\n",
    "    IdentifierList_dict['index'] = dict2['index']\n",
    "    return IdentifierList_dict\n",
    "\n",
    "\n",
    "def IdentifierList2(i):\n",
    "    IdentifierList2_dict = dict()\n",
    "    IdentifierList2_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        if temp['token_type'] == Token_type.Comma:\n",
    "            match1 = Match(Token_type.Comma, i)\n",
    "            IdentifierList2_children.append(match1['node'])\n",
    "            match2 = Match(Token_type.identifier, match1['index'])\n",
    "            IdentifierList2_children.append(match2['node'])\n",
    "            dict3 = IdentifierList2(match2['index'])\n",
    "            IdentifierList2_children.append(dict3['node'])\n",
    "            last_index = dict3['index']\n",
    "\n",
    "        else:\n",
    "            IdentifierList2_dict['node'] = None\n",
    "            IdentifierList2_dict['index'] = i\n",
    "            return IdentifierList2_dict\n",
    "\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        IdentifierList2_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    IdentifierList2_node = Tree(\"IdentifierList2\", IdentifierList2_children)\n",
    "    IdentifierList2_dict['node'] = IdentifierList2_node\n",
    "    IdentifierList2_dict['index'] = last_index\n",
    "    return IdentifierList2_dict\n",
    "\n",
    "\n",
    "def Conditional(i):\n",
    "    Conditional_dict = dict()\n",
    "    Conditional_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        if temp['token_type'] in [Token_type.identifier, Token_type.constant]:\n",
    "            dict1 = IdorConst(i)\n",
    "            Conditional_children.append(dict1['node'])\n",
    "            dict2 = RelationalOp(dict1['index'])\n",
    "            Conditional_children.append(dict2['node'])\n",
    "            dict3 = IdorConst(dict2['index'])\n",
    "            Conditional_children.append(dict3['node'])\n",
    "            last_index = dict3['index']\n",
    "        elif temp['token_type'] == Token_type.identifier:\n",
    "            match1 = Match(Token_type.identifier, i)\n",
    "            Conditional_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] in [Token_type.true, Token_type.false]:\n",
    "            dict1 = LogicalVal(i)\n",
    "            Conditional_children.append(dict1['node'])\n",
    "            last_index = dict1['index']\n",
    "        else:\n",
    "            match1 = Match(Token_type.Error, i)\n",
    "            Conditional_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        Conditional_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    Conditional_node = Tree(\"Conditional\", Conditional_children)\n",
    "    Conditional_dict['node'] = Conditional_node\n",
    "    Conditional_dict['index'] = last_index\n",
    "    return Conditional_dict\n",
    "\n",
    "\n",
    "def IdorConst(i):\n",
    "    IdorConst_dict = dict()\n",
    "    IdorConst_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        if temp['token_type'] == Token_type.identifier:\n",
    "            match1 = Match(Token_type.identifier, i)\n",
    "            IdorConst_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] == Token_type.constant:\n",
    "            match1 = Match(Token_type.constant, i)\n",
    "            IdorConst_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "\n",
    "        else:\n",
    "            match1 = Match(Token_type.Error, i)\n",
    "            IdorConst_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        IdorConst_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    IdorConst_node = Tree(\"IdorConst\", IdorConst_children)\n",
    "    IdorConst_dict['node'] = IdorConst_node\n",
    "    IdorConst_dict['index'] = last_index\n",
    "    return IdorConst_dict\n",
    "\n",
    "\n",
    "def RelationalOp(i):\n",
    "    RelationalOp_dict = dict()\n",
    "    RelationalOp_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        if temp['token_type'] == Token_type.GreaterThanOp:\n",
    "            match1 = Match(Token_type.GreaterThanOp, i)\n",
    "            RelationalOp_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] == Token_type.LessThanOp:\n",
    "            match1 = Match(Token_type.LessThanOp, i)\n",
    "            RelationalOp_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] == Token_type.LessThanEqualOp:\n",
    "            match1 = Match(Token_type.LessThanEqualOp, i)\n",
    "            RelationalOp_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] == Token_type.GreaterThanEqualOp:\n",
    "            match1 = Match(Token_type.GreaterThanEqualOp, i)\n",
    "            RelationalOp_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] == Token_type.EqualEqualOp:\n",
    "            match1 = Match(Token_type.EqualEqualOp, i)\n",
    "            RelationalOp_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] == Token_type.NotEqualOp:\n",
    "            match1 = Match(Token_type.NotEqualOp, i)\n",
    "            RelationalOp_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        else:\n",
    "            match1 = Match(Token_type.Error, i)\n",
    "            RelationalOp_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        RelationalOp_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    RelationalOp_node = Tree(\"RelationalOp\", RelationalOp_children)\n",
    "    RelationalOp_dict['node'] = RelationalOp_node\n",
    "    RelationalOp_dict['index'] = last_index\n",
    "    return RelationalOp_dict\n",
    "\n",
    "\n",
    "def LogicalOrIdentifier(i):\n",
    "    LogicalOrIdentifier_dict = dict()\n",
    "    LogicalOrIdentifier_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        if temp['token_type'] in [Token_type.true, Token_type.false]:\n",
    "            dict1 = LogicalVal(i)\n",
    "            LogicalOrIdentifier_children.append(dict1['node'])\n",
    "            last_index = dict1['index']\n",
    "        elif temp['token_type'] == Token_type.identifier:\n",
    "            dict1 = IdentifierList(i)\n",
    "            LogicalOrIdentifier_children.append(dict1['node'])\n",
    "            last_index = dict1['index']\n",
    "        else:\n",
    "            match1 = Match(Token_type.Error, i)\n",
    "            LogicalOrIdentifier_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        LogicalOrIdentifier_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    LogicalOrIdentifier_node = Tree(\n",
    "        \"LogicalOrIdentifier\", LogicalOrIdentifier_children)\n",
    "    LogicalOrIdentifier_dict['node'] = LogicalOrIdentifier_node\n",
    "    LogicalOrIdentifier_dict['index'] = last_index\n",
    "    return LogicalOrIdentifier_dict\n",
    "\n",
    "\n",
    "def LogicalOrConst(i):\n",
    "    LogicalOrConst_dict = dict()\n",
    "    LogicalOrConst_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        if temp['token_type'] in [Token_type.true, Token_type.false]:\n",
    "            dict1 = LogicalVal(i)\n",
    "            LogicalOrConst_children.append(dict1['node'])\n",
    "            last_index = dict1['index']\n",
    "        elif temp['token_type'] == Token_type.constant:\n",
    "            match1 = Match(Token_type.constant, i)\n",
    "            LogicalOrConst_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        else:\n",
    "            match1 = Match(Token_type.Error, i)\n",
    "            LogicalOrConst_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        LogicalOrConst_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    LogicalOrConst_node = Tree(\"LogicalOrConst\", LogicalOrConst_children)\n",
    "    LogicalOrConst_dict['node'] = LogicalOrConst_node\n",
    "    LogicalOrConst_dict['index'] = last_index\n",
    "    return LogicalOrConst_dict\n",
    "\n",
    "\n",
    "def LogicalVal(i):\n",
    "    LogicalVal_dict = dict()\n",
    "    LogicalVal_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        temp = Tokens[i].to_dict()\n",
    "        if temp['token_type'] == Token_type.true:\n",
    "            match1 = Match(Token_type.true, i)\n",
    "            LogicalVal_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        elif temp['token_type'] == Token_type.false:\n",
    "            match1 = Match(Token_type.false, i)\n",
    "            LogicalVal_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "        else:\n",
    "            match1 = Match(Token_type.Error, i)\n",
    "            LogicalVal_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        LogicalVal_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "    LogicalVal_node = Tree(\"LogicalVal\", LogicalVal_children)\n",
    "    LogicalVal_dict['node'] = LogicalVal_node\n",
    "    LogicalVal_dict['index'] = last_index\n",
    "    return LogicalVal_dict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GUI\n",
    "root= tk.Tk()\n",
    "canvas1 = tk.Canvas(root, width=400, height=300, relief='raised')\n",
    "canvas1.pack()\n",
    "label1 = tk.Label(root, text='Scanner Phase')\n",
    "label1.config(font=('helvetica', 14))\n",
    "canvas1.create_window(200, 25, window=label1)\n",
    "label2 = tk.Label(root, text='Source code:')\n",
    "label2.config(font=('helvetica', 10))\n",
    "canvas1.create_window(200, 100, window=label2)\n",
    "\n",
    "entry1 = tk.Entry(root) \n",
    "canvas1.create_window(200, 140, window=entry1)\n",
    "\n",
    "def Scan():\n",
    "    x1 = entry1.get()\n",
    "    find_token(x1)\n",
    "    df=pandas.DataFrame.from_records([t.to_dict() for t in Tokens])\n",
    "    #print(df)\n",
    "      \n",
    "    #to display token stream as table\n",
    "    dTDa1 = tk.Toplevel()\n",
    "    dTDa1.title('Token Stream')\n",
    "    dTDaPT = pt.Table(dTDa1, dataframe=df, showtoolbar=True, showstatusbar=True)\n",
    "    dTDaPT.show()\n",
    "    # start Parsing\n",
    "    # Node=Parse()\n",
    "    Node=ProgramStart()\n",
    "     \n",
    "    \n",
    "    # to display errorlist\n",
    "    df1=pandas.DataFrame(Errors)\n",
    "    dTDa2 = tk.Toplevel()\n",
    "    dTDa2.title('Error List')\n",
    "    dTDaPT2 = pt.Table(dTDa2, dataframe=df1, showtoolbar=True, showstatusbar=True)\n",
    "    dTDaPT2.show()\n",
    "    Node.draw()\n",
    "    #clear your list\n",
    "    \n",
    "    #label3 = tk.Label(root, text='Lexem ' + x1 + ' is:', font=('helvetica', 10))\n",
    "    #canvas1.create_window(200, 210, window=label3)\n",
    "    \n",
    "    #label4 = tk.Label(root, text=\"Token_type\"+x1, font=('helvetica', 10, 'bold'))\n",
    "    #canvas1.create_window(200, 230, window=label4)\n",
    "    \n",
    "    \n",
    "button1 = tk.Button(text='Scan', command=Scan, bg='brown', fg='white', font=('helvetica', 9, 'bold'))\n",
    "canvas1.create_window(200, 180, window=button1)\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
