{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from enum import Enum\n",
    "import re\n",
    "import pandas\n",
    "import pandastable as pt\n",
    "from nltk.tree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token_type(Enum):  # listing all tokens type\n",
    "    program = 1\n",
    "    implicit = 2\n",
    "    none = 3\n",
    "    integer = 4\n",
    "    real = 5\n",
    "    Complex = 6\n",
    "    logical = 7\n",
    "    Dot = 8\n",
    "    Semicolon = 9\n",
    "    EqualOp = 10\n",
    "    LessThanOp = 11\n",
    "    GreaterThanOp = 12\n",
    "    NotEqualOp = 13\n",
    "    PlusOp = 14\n",
    "    MinusOp = 15\n",
    "    MultiplyOp = 16\n",
    "    DivideOp = 17\n",
    "    VarDeclOp = 18\n",
    "    character = 19\n",
    "    ExclMark = 20\n",
    "    parameter = 21\n",
    "    end = 22\n",
    "    If = 23\n",
    "    then = 24\n",
    "    Else = 25\n",
    "    do = 26\n",
    "    string = 27\n",
    "    read = 28\n",
    "    Print = 29\n",
    "    LessThanEqualOp = 30\n",
    "    GreaterThanEqualOp = 31\n",
    "    EqualEqualOp = 32\n",
    "    constant = 33\n",
    "    identifier = 34\n",
    "    Error = 35\n",
    "    Comma = 36\n",
    "    Len = 37\n",
    "    openParenthesis=38\n",
    "    closeParenthesis=39\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserved word Dictionary\n",
    "ReservedWords = {\n",
    "    \"program\": Token_type.program,\n",
    "    \"implicit\": Token_type.implicit,\n",
    "    \"none\": Token_type.none,\n",
    "    \"end\": Token_type.end,\n",
    "    \"integer\": Token_type.integer,\n",
    "    \"real\": Token_type.real,\n",
    "    \"complex\": Token_type.Complex,\n",
    "    \"logical\": Token_type.logical,\n",
    "    \"character\": Token_type.character,\n",
    "    \"parameter\": Token_type.parameter,\n",
    "    \"if\": Token_type.If,\n",
    "    \"then\": Token_type.then,\n",
    "    \"else\": Token_type.Else,\n",
    "    \"do\": Token_type.do,\n",
    "    \"read\": Token_type.read,\n",
    "    \"print\": Token_type.Print,\n",
    "    \"len\": Token_type.Len\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Operators = {\n",
    "    \".\": Token_type.Dot,\n",
    "    # \";\": Token_type.Semicolon,\n",
    "    \"=\": Token_type.EqualOp,\n",
    "    \"+\": Token_type.PlusOp,\n",
    "    \"-\": Token_type.MinusOp,\n",
    "    \"*\": Token_type.MultiplyOp,\n",
    "    \"/\": Token_type.DivideOp,\n",
    "    \"::\": Token_type.VarDeclOp,\n",
    "    \"!\": Token_type.ExclMark,\n",
    "    \">\": Token_type.GreaterThanOp,\n",
    "    \"<\": Token_type.LessThanOp,\n",
    "    \"<=\": Token_type.LessThanEqualOp,\n",
    "    \">=\": Token_type.GreaterThanEqualOp,\n",
    "    \"/=\": Token_type.NotEqualOp,\n",
    "    \"==\": Token_type.EqualEqualOp,\n",
    "    \",\": Token_type.Comma,\n",
    "    \"(\":Token_type.openParenthesis,\n",
    "    \")\":Token_type.closeParenthesis\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operator Precedence\n",
    "# '*'    '/'    '+'    '-'    '>'    '<'    '<='    '>='    '=='    '/='\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class token to hold string and token type\n",
    "class token:\n",
    "    def __init__(self, lex, token_type):\n",
    "        self.lex = lex\n",
    "        self.token_type = token_type\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'Lex': self.lex,\n",
    "            'token_type': self.token_type\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokens=[]\n",
    "Errors=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_token(text):\n",
    "    lexems = text.split()\n",
    "    for le in lexems:\n",
    "        if (le in ReservedWords):\n",
    "            new_token = token(le, ReservedWords[le])\n",
    "            Tokens.append(new_token)\n",
    "        elif (le in Operators):\n",
    "            new_token = token(le, Operators[le])\n",
    "            Tokens.append(new_token)\n",
    "        elif (re.match(\"^\\d+(\\.[0-9]*)?$\", le)):\n",
    "            new_token = token(le, Token_type.constant)\n",
    "            Tokens.append(new_token)\n",
    "        elif (re.match(\"^([a-zA-Z][a-zA-Z0-9]*)$\", le)):\n",
    "            new_token = token(le, Token_type.identifier)\n",
    "            Tokens.append(new_token)\n",
    "        elif (re.match(\"^\\\"[\\w. ]+\\\"$\", le) or re.match(\"^\\'[\\w. ]+\\'$\", le)):\n",
    "            new_token = token(le, Token_type.string)\n",
    "            Tokens.append(new_token)\n",
    "        else:\n",
    "            new_token = token(le, Token_type.Error)\n",
    "            Errors.append(\"Lexical error  \" + le)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Match(TT,i) :\n",
    "    out = dict()\n",
    "    if(i<len(Tokens)) :\n",
    "        TokDict=Tokens[i].to_dict()\n",
    "        if(TokDict['token_type'] == TT) :\n",
    "            i+=1\n",
    "            out['node'] = [TokDict['Lex']]\n",
    "            out['index']=i\n",
    "            return out\n",
    "        else:\n",
    "            out['node']=['error']\n",
    "            out['index']=i+1\n",
    "            Errors.append(\"Syntax Error: \"+TT['Lex'])\n",
    "            return out\n",
    "    else :\n",
    "        out['node']=['error']\n",
    "        out['index']=i+1\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parse():\n",
    "    i = 0\n",
    "    ParseChildren = []\n",
    "    ProgramStartDict = ProgramStart(i)\n",
    "\n",
    "\n",
    "def ProgramStart(i):\n",
    "    ProgramStart_Dict = dict()\n",
    "    ProgramStart_Children = []\n",
    "    Dict1 = ProgramUnit(i)\n",
    "    ProgramStart_Children.append(Dict1['node'])\n",
    "    Dict2 = ProgramStart2(Dict1['index'])\n",
    "    ProgramStart_Children.append(Dict2['node'])\n",
    "    ProgramStart_Node = Tree(\"ProgramStart\", ProgramStart_Children)\n",
    "    ProgramStart_Dict['node'] = ProgramStart_Node\n",
    "    ProgramStart_Dict['index'] = Dict2['index']\n",
    "    return ProgramStart_Dict\n",
    "\n",
    "\n",
    "def ProgramStart2(i):\n",
    "    ProgramStart2_Dict = dict()\n",
    "    ProgramStart2_Children = []\n",
    "    if i < len(Tokens):\n",
    "        Temp = Tokens[i].to_dict()\n",
    "        if Temp['token_type'] == Token_type.program:\n",
    "            Dict1 = ProgramUnit(i)\n",
    "            ProgramStart2_Children.append(Dict1['node'])\n",
    "            Dict2 = ProgramStart2(Dict1['index'])\n",
    "            ProgramStart2_Children.append(Dict2['node'])\n",
    "            ProgramStart2_Node = Tree(\"ProgramStart2\", ProgramStart2_Children)\n",
    "            ProgramStart2_Dict['node'] = ProgramStart2_Node\n",
    "            ProgramStart2_Dict['index'] = Dict2['index']\n",
    "            return ProgramStart2_Dict\n",
    "        else:\n",
    "            match1 = Match(Token_type.Error, i)\n",
    "            ProgramStart2_Children.append(match1['node'])\n",
    "            ProgramStart2_Node = Tree(\"ProgramStart2\", ProgramStart2_Children)\n",
    "            ProgramStart2_Dict['node'] = ProgramStart2_Node\n",
    "            ProgramStart2_Dict['index'] = match1['index']\n",
    "            return ProgramStart2_Dict\n",
    "\n",
    "    else:\n",
    "\n",
    "        ProgramStart2_Dict['node'] = None\n",
    "        ProgramStart2_Dict['index'] = i\n",
    "        return ProgramStart2_Dict\n",
    "\n",
    "\n",
    "def ProgramUnit(i):\n",
    "    ProgramUnit_dict = dict()\n",
    "    ProgramUnit_children = []\n",
    "    dict1 = Header(i)\n",
    "    ProgramUnit_children.append(dict1['node'])\n",
    "    dict2 = Block(dict1['index'])\n",
    "    ProgramUnit_children.append(dict2['node'])\n",
    "    dict3 = Footer(dict2['index'])\n",
    "    ProgramUnit_children.append(dict3['node'])\n",
    "    ProgramUnit_node = Tree(\"ProgramUnit\", ProgramUnit_children)\n",
    "    ProgramUnit_dict['node'] = ProgramUnit_node\n",
    "    ProgramUnit_dict['index'] = dict3['index']\n",
    "    return ProgramUnit_dict\n",
    "\n",
    "\n",
    "def Header(i):\n",
    "    Header_dict = dict()\n",
    "    Header_children = []\n",
    "    match1 = Match(Token_type.program, i)\n",
    "    Header_children.append(match1['node'])\n",
    "    match2 = Match(Token_type.identifier, match1['index'])\n",
    "    Header_children.append(match2['node'])\n",
    "    Header_node = Tree(\"Header\", Header_children)\n",
    "    Header_dict['node'] = Header_node\n",
    "    Header_dict['index'] = match2['index']\n",
    "    return Header_dict\n",
    "\n",
    "\n",
    "def Block(i):\n",
    "    Block_dict = dict()\n",
    "    Block_children = []\n",
    "    match1 = Match(Token_type.implicit, i)\n",
    "    Block_children.append(match1['node'])\n",
    "    match2 = Match(Token_type.none, match1['index'])\n",
    "    Block_children.append(match2['node'])\n",
    "    dict3 = TypeDecls(match2['index'])\n",
    "    Block_children.append(dict3['node'])\n",
    "    dict4 = Statements(dict3['index'])\n",
    "    Block_children.append(dict4['node'])\n",
    "    Block_node = Tree(\"Block\", Block_children)\n",
    "    Block_dict['node'] = Block_node\n",
    "    Block_dict['index'] = dict4['index']\n",
    "    return Block_dict\n",
    "\n",
    "\n",
    "def Footer(i):\n",
    "    Footer_dict = dict()\n",
    "    Footer_children = []\n",
    "    match1 = Match(Token_type.end, i)\n",
    "    Footer_children.append(match1['node'])\n",
    "    match2 = Match(Token_type.program, match1['index'])\n",
    "    Footer_children.append(match2['node'])\n",
    "    match3 = Match(Token_type.identifier, match2['index'])\n",
    "    Footer_children.append(match3['node'])\n",
    "    Footer_node = Tree(\"Footer\", Footer_children)\n",
    "    Footer_dict['node'] = Footer_node\n",
    "    Footer_dict['index'] = match3['index']\n",
    "    return Footer_dict\n",
    "\n",
    "\n",
    "def TypeDecls(i):\n",
    "    TypeDecls_dict = dict()\n",
    "    TypeDecls_children = []\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        Temp = Tokens[i].to_dict()\n",
    "        ############################################\n",
    "        if Temp['token_type'] in [Token_type.integer, Token_type.real, Token_type.Complex, Token_type.logical, Token_type.character]:\n",
    "            dict1 = TypeDecl(i)\n",
    "            TypeDecls_children.append(dict1['node'])\n",
    "            dict2 = TypeDecls2(dict1['index'])\n",
    "            TypeDecls_children.append(dict2['node'])\n",
    "            last_index = dict2['index']\n",
    "        else:\n",
    "            match1 = Match(Token_type.Error, i)\n",
    "            TypeDecls_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        TypeDecls_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "\n",
    "    TypeDecls_node = Tree(\"TypeDecls\", TypeDecls_children)\n",
    "    TypeDecls_dict['node'] = TypeDecls_node\n",
    "    TypeDecls_dict['index'] = last_index\n",
    "    return TypeDecls_dict\n",
    "\n",
    "\n",
    "def TypeDecls2(i):\n",
    "    TypeDecls2_dict = dict()\n",
    "    TypeDecls2_children=[]\n",
    "    last_index = i\n",
    "    if i < len(Tokens):\n",
    "        Temp = Tokens[i].to_dict()\n",
    "        ############################################\n",
    "        if Temp['token_type'] in [Token_type.integer, Token_type.real, Token_type.Complex, Token_type.logical, Token_type.character]:\n",
    "            dict1 = TypeDecl(i)\n",
    "            TypeDecls2_children.append(dict1['node'])\n",
    "            dict2 = TypeDecls2(dict1['index'])\n",
    "            TypeDecls2_children.append(dict2['node'])\n",
    "            last_index = dict2['index']\n",
    "        else:\n",
    "            match1 = Match(Token_type.Error, i)\n",
    "            TypeDecls2_children.append(match1['node'])\n",
    "            last_index = match1['index']\n",
    "    else:\n",
    "        match1 = Match(Token_type.Error, i)\n",
    "        TypeDecls2_children.append(match1['node'])\n",
    "        last_index = match1['index']\n",
    "\n",
    "    TypeDecls_node = Tree(\"TypeDecls\", TypeDecls2_children)\n",
    "    TypeDecls2_dict['node'] = TypeDecls_node\n",
    "    TypeDecls2_dict['index'] = last_index\n",
    "    return TypeDecls2_dict\n",
    "\n",
    "def TypeDecl(i):\n",
    "    TypeDecl_dict=dict()\n",
    "    TypeDecl_children=[]\n",
    "    dict1=DataType(i)\n",
    "    TypeDecl_children.append(dict1['node'])\n",
    "    dict2=TypeDecl2(dict1['index'])\n",
    "    TypeDecl_children.append(dict2['node'])\n",
    "    TypeDecl_node=Tree(\"TypeDecl\",TypeDecl_children)\n",
    "    TypeDecl_dict['node']=TypeDecl_node\n",
    "    TypeDecl_dict['index']=dict2['index']\n",
    "    return TypeDecl_dict\n",
    "\n",
    "def TypeDecl2(i):\n",
    "    TypeDecl2_dict=dict()\n",
    "    TypeDecl2_children=[]\n",
    "    last_index=i\n",
    "    if i < len(Tokens) :\n",
    "        temp=Tokens[i].to_dict()\n",
    "        if temp['token_type'] == Token_type.VarDeclOp :\n",
    "            match1=Match(Token_type.VarDeclOp,i)\n",
    "            TypeDecl2_children.append(match1['node'])\n",
    "            dict2=IdentifierList(match1['index'])\n",
    "            TypeDecl2_children.append(dict2['node'])\n",
    "            last_index=dict2['index']\n",
    "        elif temp['token_type'] == Token_type.Comma :\n",
    "            match1=Match(Token_type.Comma,i)\n",
    "            TypeDecl2_children.append(match1['node'])\n",
    "            match2=Match(Token_type.parameter,match1['index'])\n",
    "            TypeDecl2_children.append(match2['node'])\n",
    "            match3=Match(Token_type.VarDeclOp,match2['index'])\n",
    "            TypeDecl2_children.append(match3['node'])\n",
    "            dict4=NamedConstant(match3['index'])\n",
    "            TypeDecl2_children.append(dict4['node'])\n",
    "            last_index=dict4['index']\n",
    "        else :\n",
    "            match1=Match(Token_type.Error,i)\n",
    "            TypeDecl2_children.append(match1['node'])\n",
    "            last_index=match1['index']\n",
    "    TypeDecl2_node=Tree(\"TypeDecl2\",TypeDecl2_children)\n",
    "    TypeDecl2_dict['node']=TypeDecl2_node\n",
    "    TypeDecl2_dict['index']=last_index\n",
    "    return TypeDecl2_dict\n",
    "\n",
    "def DataType(i):\n",
    "    DataType_dict=dict()\n",
    "    DataType_children=[]\n",
    "    last_index=i\n",
    "    if i < len(Tokens) :\n",
    "        temp=Tokens[i].to_dict()\n",
    "        if temp['token_type']==Token_type.integer:\n",
    "            match1=Match(Token_type.integer,i)\n",
    "            DataType_children.append(match1['node'])\n",
    "            last_index=match1['index']\n",
    "        elif temp['token_type']==Token_type.real:\n",
    "            match1=Match(Token_type.real,i)\n",
    "            DataType_children.append(match1['node'])\n",
    "            last_index=match1['index']\n",
    "        elif temp['token_type']==Token_type.Complex:\n",
    "            match1=Match(Token_type.Complex,i)\n",
    "            DataType_children.append(match1['node'])\n",
    "            last_index=match1['index']\n",
    "        elif temp['token_type']==Token_type.logical:\n",
    "            match1=Match(Token_type.logical,i)\n",
    "            DataType_children.append(match1['node'])\n",
    "            last_index=match1['index']\n",
    "        elif temp['token_type']==Token_type.character:\n",
    "            dict1=CharacterDType(i)\n",
    "            DataType_children.append(dict1['node'])\n",
    "            last_index=dict1['index']\n",
    "        else :\n",
    "            match1=Match(Token_type.Error,i)\n",
    "            DataType_children.append(match1['node'])\n",
    "            last_index=match1['index']\n",
    "\n",
    "    else :\n",
    "        match1=Match(Token_type.Error,i)\n",
    "        DataType_children.append(match1['node'])\n",
    "        last_index=match1['index']\n",
    "    DataType_node=Tree(\"DataType\",DataType_children)\n",
    "    DataType_dict['node']=DataType_node\n",
    "    DataType_dict['index']=last_index\n",
    "    return DataType_dict\n",
    "\n",
    "def NamedConstant(i):\n",
    "    NamedConstant_dict=dict()\n",
    "    NamedConstant_children=[]\n",
    "    match1=Match(Token_type.identifier,i)\n",
    "    NamedConstant_children.append(match1['node'])\n",
    "    match2=Match(Token_type.EqualOp,match1['index'])\n",
    "    NamedConstant_children.append(match2['node'])\n",
    "    match3=Match(Token_type.constant,match2['index'])\n",
    "    NamedConstant_children.append(match3['node'])\n",
    "    NamedConstant_node=Tree(\"NamedConstant\",NamedConstant_children)\n",
    "    NamedConstant_dict['node']=NamedConstant_node\n",
    "    NamedConstant_dict['index']=match3['index']\n",
    "    return NamedConstant_dict\n",
    "\n",
    "def CharacterDType(i):\n",
    "    CharacterDType_dict=dict()\n",
    "    CharacterDType_children=[]\n",
    "    match1=Match(Token_type.character,i)\n",
    "    CharacterDType_children.append(match1['node'])\n",
    "    dict2=CharacterDType2(match1['index'])\n",
    "    CharacterDType_children.append(dict2['node'])\n",
    "    CharacterDType_node=Tree(\"CharacterDType\",CharacterDType_children)\n",
    "    CharacterDType_dict['node']=CharacterDType_node\n",
    "    CharacterDType_dict['index']=dict2['index']\n",
    "    return CharacterDType_dict\n",
    "\n",
    "def CharacterDType2(i):\n",
    "    CharacterDType2_dict=dict()\n",
    "    CharacterDType2_children=[]\n",
    "    last_index=i\n",
    "    if i < len(Tokens):\n",
    "        temp=Tokens[i].to_dict()\n",
    "        if temp['token_type']==Token_type.openParenthesis :\n",
    "            match1=Match(Token_type.openParenthesis,i)\n",
    "            CharacterDType2_children.append(match1['node'])\n",
    "            match2=Match(Token_type.Len,match1['index'])\n",
    "            CharacterDType2_children.append(match2['node'])\n",
    "            match3=Match(Token_type.EqualOp,match2['index'])\n",
    "            CharacterDType2_children.append(match3['node'])\n",
    "            dict4=IdorConst(match3['index'])\n",
    "            CharacterDType2_children.append(dict4['node'])\n",
    "            match5=Match(Token_type.closeParenthesis,dict4['index'])\n",
    "            CharacterDType2_children.append(match5['node'])\n",
    "            last_index=match5['index']\n",
    "        else :\n",
    "            CharacterDType2_dict['node']=None\n",
    "            CharacterDType2_dict['index']=i\n",
    "            return CharacterDType2_dict\n",
    "    else :\n",
    "        match1=Match(Token_type.Error,i)\n",
    "        CharacterDType2_children.append(match1['node'])\n",
    "        last_index=match1['index']\n",
    "    CharacterDType2_node=Tree(\"CharacterDType2\",CharacterDType2_children)\n",
    "    CharacterDType2_dict['node']=CharacterDType2_node\n",
    "    CharacterDType2_dict['index']=last_index\n",
    "    return CharacterDType2_dict\n",
    "\n",
    "def Statements(i):\n",
    "    Statements_dict=dict()\n",
    "    Statements_children=[]\n",
    "    dict1=Statement(i)\n",
    "    Statements_children.append(dict1['node'])\n",
    "    dict2=Statements2(dict1['index'])\n",
    "    Statements_children.append(dict2['node'])\n",
    "    Statements_node=Tree(\"Statements\",Statements_children)\n",
    "    Statements_dict['node']=Statements_node\n",
    "    Statements_dict['index']=dict2['index']\n",
    "    return Statements_dict\n",
    "\n",
    "def Statements2(i):\n",
    "    Statements2_dict=dict()\n",
    "    Statements2_children=[]\n",
    "    last_index=i\n",
    "    if i < len(Tokens):\n",
    "        temp=Tokens[i].to_dict()\n",
    "        #################################################\n",
    "        if temp['token_type'] in [Token_type.identifier,Token_type.Print,Token_type.read,Token_type.If,Token_type.do] :\n",
    "            dict1=Statement(i)\n",
    "            Statements2_children.append(dict1['node'])\n",
    "            dict2=Statements2(dict1['index'])\n",
    "            Statements2_children.append(dict2['node'])\n",
    "            last_index=dict2['index']\n",
    "            \n",
    "        else :\n",
    "            Statements2_dict['node']=None\n",
    "            Statements2_dict['index']=i\n",
    "            return Statements2_dict\n",
    "    else :\n",
    "        match1=Match(Token_type.Error,i)\n",
    "        Statements2_children.append(match1['node'])\n",
    "        last_index=match1['index']\n",
    "    Statements2_node=Tree(\"Statements2\",Statements2_children)\n",
    "    Statements2_dict['node']=Statements2_node\n",
    "    Statements2_dict['index']=last_index\n",
    "    return Statements2_dict\n",
    "\n",
    "def Statement(i) :\n",
    "    Statement_dict=dict()\n",
    "    Statement_children=[]\n",
    "    last_index=i\n",
    "    if i < len(Tokens):\n",
    "        temp=Tokens[i].to_dict()\n",
    "        if temp['token_type']==Token_type.identifier :\n",
    "            dict1=Assignment(i)\n",
    "            Statement_children.append(dict1['node'])\n",
    "            last_index=dict1['index']\n",
    "        elif temp['token_type']==Token_type.Print :\n",
    "            dict1=Print(i)\n",
    "            Statement_children.append(dict1['node'])\n",
    "            last_index=dict1['index']\n",
    "        elif temp['token_type']==Token_type.read :\n",
    "            dict1=Read(i)\n",
    "            Statement_children.append(dict1['node'])\n",
    "            last_index=dict1['index']\n",
    "        elif temp['token_type']\n",
    "        else :\n",
    "            Statement_dict['node']=None\n",
    "            Statement_dict['index']=i\n",
    "            return Statement_dict\n",
    "    else :\n",
    "        match1=Match(Token_type.Error,i)\n",
    "        Statement_children.append(match1['node'])\n",
    "        last_index=match1['index']\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GUI\n",
    "root= tk.Tk()\n",
    "canvas1 = tk.Canvas(root, width=400, height=300, relief='raised')\n",
    "canvas1.pack()\n",
    "label1 = tk.Label(root, text='Scanner Phase')\n",
    "label1.config(font=('helvetica', 14))\n",
    "canvas1.create_window(200, 25, window=label1)\n",
    "label2 = tk.Label(root, text='Source code:')\n",
    "label2.config(font=('helvetica', 10))\n",
    "canvas1.create_window(200, 100, window=label2)\n",
    "\n",
    "entry1 = tk.Entry(root) \n",
    "canvas1.create_window(200, 140, window=entry1)\n",
    "\n",
    "def Scan():\n",
    "    x1 = entry1.get()\n",
    "    find_token(x1)\n",
    "    df=pandas.DataFrame.from_records([t.to_dict() for t in Tokens])\n",
    "    #print(df)\n",
    "      \n",
    "    #to display token stream as table\n",
    "    dTDa1 = tk.Toplevel()\n",
    "    dTDa1.title('Token Stream')\n",
    "    dTDaPT = pt.Table(dTDa1, dataframe=df, showtoolbar=True, showstatusbar=True)\n",
    "    dTDaPT.show()\n",
    "    # start Parsing\n",
    "    Node=Parse()\n",
    "     \n",
    "    \n",
    "    # to display errorlist\n",
    "    df1=pandas.DataFrame(errors)\n",
    "    dTDa2 = tk.Toplevel()\n",
    "    dTDa2.title('Error List')\n",
    "    dTDaPT2 = pt.Table(dTDa2, dataframe=df1, showtoolbar=True, showstatusbar=True)\n",
    "    dTDaPT2.show()\n",
    "    Node.draw()\n",
    "    #clear your list\n",
    "    \n",
    "    #label3 = tk.Label(root, text='Lexem ' + x1 + ' is:', font=('helvetica', 10))\n",
    "    #canvas1.create_window(200, 210, window=label3)\n",
    "    \n",
    "    #label4 = tk.Label(root, text=\"Token_type\"+x1, font=('helvetica', 10, 'bold'))\n",
    "    #canvas1.create_window(200, 230, window=label4)\n",
    "    \n",
    "    \n",
    "button1 = tk.Button(text='Scan', command=Scan, bg='brown', fg='white', font=('helvetica', 9, 'bold'))\n",
    "canvas1.create_window(200, 180, window=button1)\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
